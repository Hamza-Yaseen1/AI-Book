"use strict";(globalThis.webpackChunkphysical_ai=globalThis.webpackChunkphysical_ai||[]).push([[6494],{6944:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ch03-ml-physical-ai/ch03-lesson03/index","title":"Lesson 3: Reinforcement Learning","description":"Introduction to Reinforcement Learning in Physical Systems","source":"@site/docs/ch03-ml-physical-ai/ch03-lesson03/index.md","sourceDirName":"ch03-ml-physical-ai/ch03-lesson03","slug":"/ch03-ml-physical-ai/ch03-lesson03/","permalink":"/docs/ch03-ml-physical-ai/ch03-lesson03/","draft":false,"unlisted":false,"editUrl":"https://github.com/hamza-11/physical-ai/tree/main/docs/ch03-ml-physical-ai/ch03-lesson03/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2: Computer Vision","permalink":"/docs/ch03-ml-physical-ai/ch03-lesson02/"},"next":{"title":"Lesson 1: Smart Home Automation","permalink":"/docs/ch04-physical-ai-applications/ch04-lesson01/"}}');var t=i(4848),l=i(8453);const a={sidebar_position:3},o="Lesson 3: Reinforcement Learning",r={},c=[{value:"Introduction to Reinforcement Learning in Physical Systems",id:"introduction-to-reinforcement-learning-in-physical-systems",level:2},{value:"Q-Learning Fundamentals",id:"q-learning-fundamentals",level:2},{value:"Key Concepts:",id:"key-concepts",level:3},{value:"Line-Following Robot Application",id:"line-following-robot-application",level:2},{value:"Implementation Components:",id:"implementation-components",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Key Challenges:",id:"key-challenges",level:3},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Steps:",id:"steps",level:3},{value:"Summary",id:"summary",level:2}];function h(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-3-reinforcement-learning",children:"Lesson 3: Reinforcement Learning"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-reinforcement-learning-in-physical-systems",children:"Introduction to Reinforcement Learning in Physical Systems"}),"\n",(0,t.jsx)(n.p,{children:"Reinforcement Learning (RL) provides a framework for training agents to make sequential decisions in physical environments. Unlike supervised learning, RL learns through interaction with the environment, making it ideal for control tasks where the optimal policy is not known a priori."}),"\n",(0,t.jsx)(n.h2,{id:"q-learning-fundamentals",children:"Q-Learning Fundamentals"}),"\n",(0,t.jsx)(n.p,{children:"Q-learning is a model-free reinforcement learning algorithm that learns the value of actions in particular states. It's particularly useful for discrete action spaces in physical systems."}),"\n",(0,t.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"State-action value function (Q-function)"}),"\n",(0,t.jsx)(n.li,{children:"Exploration vs. exploitation trade-off"}),"\n",(0,t.jsx)(n.li,{children:"Reward signal design for physical systems"}),"\n",(0,t.jsx)(n.li,{children:"Convergence properties and stability"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"line-following-robot-application",children:"Line-Following Robot Application"}),"\n",(0,t.jsx)(n.p,{children:"A line-following robot is a classic application of reinforcement learning in physical systems. The robot learns to navigate along a marked path by receiving rewards for staying on track and penalties for deviating."}),"\n",(0,t.jsx)(n.h3,{id:"implementation-components",children:"Implementation Components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sensor array for line detection"}),"\n",(0,t.jsx)(n.li,{children:"Discrete action space (turn left, turn right, go straight)"}),"\n",(0,t.jsx)(n.li,{children:"Reward function based on line position"}),"\n",(0,t.jsx)(n.li,{children:"State representation from sensor data"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,t.jsx)(n.p,{children:"Sim-to-real transfer refers to the challenge of transferring policies learned in simulation to real-world physical systems. This is crucial for safe and efficient RL deployment."}),"\n",(0,t.jsx)(n.h3,{id:"key-challenges",children:"Key Challenges:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reality gap between simulation and real world"}),"\n",(0,t.jsx)(n.li,{children:"Domain randomization techniques"}),"\n",(0,t.jsx)(n.li,{children:"System identification and model adaptation"}),"\n",(0,t.jsx)(n.li,{children:"Safe exploration in real physical systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,t.jsx)(n.p,{children:"Implement a Q-learning algorithm for a line-following robot. The robot should learn to navigate a track by adjusting its actions based on sensor feedback and reward signals."}),"\n",(0,t.jsx)(n.h3,{id:"steps",children:"Steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a simulation environment for the line-following task"}),"\n",(0,t.jsx)(n.li,{children:"Implement Q-learning algorithm with appropriate state representation"}),"\n",(0,t.jsx)(n.li,{children:"Design reward function to encourage line-following behavior"}),"\n",(0,t.jsx)(n.li,{children:"Test policy transfer from simulation to a physical robot platform"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This lesson explored reinforcement learning applications in physical systems, focusing on Q-learning algorithms, line-following robot applications, and the challenges of sim-to-real transfer. These concepts enable physical AI systems to learn optimal behaviors through interaction with their environment."})]})}function d(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},l=s.createContext(t);function a(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);